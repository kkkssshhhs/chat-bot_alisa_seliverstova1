# –ë–æ—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è 2 –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä –ò–¢–ú–û (AI –∏ AI Product), —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º —É—á–µ–±–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤
# –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: aiogram aiohttp beautifulsoup4 rapidfuzz pdfminer.six nest_asyncio

import os
import re
import io
import json
import asyncio
import nest_asyncio
from datetime import datetime
from typing import Dict, List, Tuple, Optional

import aiohttp
from bs4 import BeautifulSoup
from rapidfuzz import fuzz, process
from pdfminer.high_level import extract_text as pdf_extract_text

from aiogram import Bot, Dispatcher
from aiogram.types import Message
from aiogram.filters import Command
from aiogram.fsm.storage.memory import MemoryStorage


nest_asyncio.apply()

# üîë –í–°–¢–ê–í–¨ —Ç–æ–∫–µ–Ω –±–æ—Ç–∞ —Ç—É—Ç
API_TOKEN = ("7653281120:AAE-pU2-GrC1ZQ8UW3S8XUySjSegxfoU1W8")

bot = Bot(token=API_TOKEN)
dp = Dispatcher(storage=MemoryStorage())

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º
PROGRAMS = {
    "ai": {
        "title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (AI Talent Hub)",
        "landing": "https://ai.itmo.ru/",
        # –∑–∞–ø–∞—Å–Ω–æ–π id —Ñ–∞–π–ª–∞ –Ω–∞ Google Drive (–µ—Å–ª–∏ –Ω–∞ –ª–µ–Ω–¥–∏–Ω–≥–µ –Ω–µ –Ω–∞—à–ª–∏ —Å—Å—ã–ª–∫—É)
        "fallback_drive_id": "1NSuBA0VkPzqNfdOD2IDR4TSDY2ea66nx",
    },
    "aiproduct": {
        "title": "AI Product (–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏)",
        "landing": "https://aiproduct.itmo.ru/",
        "fallback_drive_id": "1Uod9BdkrFqSfiCq6w2nLHQC-tPW_NNRJ",
    },
}

CACHE_PATH = "plans_cache.json"

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–µ–≥–æ–≤ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö)
TOPIC_KEYWORDS = {
    "NLP": ["nlp", "—è–∑—ã–∫", "language", "—Ç–µ–∫—Å—Ç", "bert", "gpt", "llm"],
    "CV": ["–∑—Ä–µ–Ω–∏–µ", "–∏–∑–æ–±—Ä–∞–∂", "vision", "cv", "video", "–≤–∏–¥–µ–æ"],
    "MLOps": ["mlops", "devops", "docker", "kubernetes", "cicd", "ci/cd"],
    "RecSys": ["—Ä–µ–∫–æ–º–µ–Ω–¥", "recsys", "–ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü"],
    "Graphs": ["–≥—Ä–∞—Ñ", "graph", "gnn"],
    "RL": ["–ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω", "reinforcement", "rl"],
    "GenAI": ["–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤", "gpt", "diffusion", "stable", "lora", "prompt"],
    "DataEng": ["–¥–∞–Ω–Ω", "etl", "pipeline", "spark", "hadoop", "kafka"],
    "DS/ML": ["–º–∞—à–∏–Ω", "machine", "–æ–±—É—á–µ–Ω", "ml", "–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü", "—Ä–µ–≥—Ä–µ—Å—Å", "–∫–ª–∞—Å—Ç–µ—Ä"],
    "Statistics": ["—Å—Ç–∞—Ç–∏—Å—Ç", "–≤–µ—Ä–æ—è—Ç–Ω", "hypothesis", "–∞/b", "a/b", "—Ç–µ—Å—Ç"],
    "Product": ["–ø—Ä–æ–¥—É–∫—Ç", "product", "custdev", "hypothes", "—Ä—ã–Ω–æ–∫", "go-to-market", "pm"],
    "BA/BI": ["bi", "–º–µ—Ç—Ä–∏–∫", "–∞–Ω–∞–ª–∏—Ç", "sql", "dashboard"],
}

# –£—Ç–∏–ª–∏—Ç—ã —Å–∫–∞—á–∏–≤–∞–Ω–∏—è/–ø–∞—Ä—Å–∏–Ω–≥–∞
DRIVE_REGEX = re.compile(r"https?://drive\.google\.com/file/d/([A-Za-z0-9_\-]+)/view")

def drive_download_url(file_id: str) -> str:
    return f"https://drive.google.com/uc?export=download&id={file_id}"

async def fetch_text(session: aiohttp.ClientSession, url: str) -> str:
    async with session.get(url, timeout=40) as r:
        r.raise_for_status()
        return await r.text()

async def fetch_bytes(session: aiohttp.ClientSession, url: str) -> bytes:
    async with session.get(url, timeout=120) as r:
        r.raise_for_status()
        return await r.read()

def extract_drive_id_from_html(html: str) -> Optional[str]:
    m = DRIVE_REGEX.search(html)
    return m.group(1) if m else None

def normalize_text(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()

def looks_like_course(line: str) -> bool:
    """–≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω –∏–∑ —Ç–µ–∫—Å—Ç–∞ PDF."""
    l = line.strip()
    if len(l) < 4 or len(l) > 140:
        return False

    # –æ—Ç—Å–µ–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏/–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    bad_fragments = [
        "—É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω", "—Å–µ–º–µ—Å—Ç—Ä", "–≥–æ–¥", "–∏—Ç–æ–≥–æ", "—Ñ–∞–∫—É–ª—å—Ç–∞—Ç–∏–≤",
        "–≤–∫—Ä", "–ø—Ä–∞–∫—Ç–∏–∫", "–ø—Ä–∞–∫—Ç–∏–∫–∞", "–∑–∞—á–µ—Ç", "—ç–∫–∑–∞–º–µ–Ω",
        "–∑.–µ.", "–∫—Ä–µ–¥–∏—Ç", "—Ñ–æ—Ä–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è", "–∫–∞–ª–µ–Ω–¥–∞—Ä", "–ø–ª–∞–Ω-–≥—Ä–∞—Ñ–∏–∫",
        "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –∏—Ç–º–æ", "–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω", "–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü", "–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω",
        "–º–æ–¥—É–ª—å", "—Ç—Ä—É–¥–æ–µ–º–∫", "—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å", "—á–∞—Å", "—á–∞—Å–æ–≤", "—Å–µ—Å—Å–∏—è",
        "–¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã –ø–æ –≤—ã–±–æ—Ä—É", "–≤—ã–±–æ—Ä–Ω—ã–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã",
    ]
    low = l.lower()
    if any(key in low for key in bad_fragments):
        return False

    # –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 —Å–ª–æ–≤–∞ –∏ –µ—Å—Ç—å –±—É–∫–≤—ã
    if not re.search(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë]", l):
        return False
    if len(l.split()) < 2:
        return False
        # –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∫–∞–ø—Å–æ–º –Ω–µ —Å—á–∏—Ç–∞–µ–º
    if l == l.upper() and re.search(r"[–ê-–ØA-Z]", l):
        return False

    # –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –æ—Ç—Å–µ–∫–∞–µ–º (–∫—Ä–æ–º–µ ML/NLP/CV)
    if len(l.split()) == 1 and len(l) <= 6 and l.isupper():
        return l in {"ML", "NLP", "CV"}

    return True

def parse_courses_from_pdf_bytes(pdf_bytes: bytes) -> Tuple[List[str], str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (—Å–ø–∏—Å–æ–∫ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω, —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç). –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ pdfminer.six"""
    with io.BytesIO(pdf_bytes) as bio:
        raw_text = pdf_extract_text(bio) or ""

    courses: List[str] = []
    for raw_line in raw_text.splitlines():
        line = normalize_text(raw_line)
        if looks_like_course(line):
            courses.append(line)

    # —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫
    seen = set()
    deduped = []
    for c in courses:
        key = c.lower()
        if key not in seen:
            seen.add(key)
            deduped.append(c)

    deduped = [re.sub(r"\s*[\.\:;,‚Äî-]\s*$", "", c).strip() for c in deduped]
    return deduped, raw_text

async def fetch_program_plan(session: aiohttp.ClientSession, key: str, meta: Dict) -> Dict:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç PDF ¬´–£—á–µ–±–Ω—ã–π –ø–ª–∞–Ω¬ª —á–µ—Ä–µ–∑ —Å—Å—ã–ª–∫—É —Å –ª–µ–Ω–¥–∏–Ω–≥–∞ –∏ –ø–∞—Ä—Å–∏—Ç –µ–≥–æ."""
    landing_url = meta["landing"]
    fallback_id = meta["fallback_drive_id"]

    try:
        html = await fetch_text(session, landing_url)
        
        drive_id = extract_drive_id_from_html(html) or fallback_id
    except Exception:
        drive_id = fallback_id

    pdf_url = drive_download_url(drive_id)
    pdf_bytes = await fetch_bytes(session, pdf_url)
    courses, raw_text = parse_courses_from_pdf_bytes(pdf_bytes)

    return {
        "key": key,
        "title": meta["title"],
        "landing": landing_url,
        "drive_id": drive_id,
        "pdf_url": pdf_url,
        "courses": courses,
        "raw_text": raw_text,
        "updated_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
    }

async def sync_all() -> Dict[str, Dict]:
    async with aiohttp.ClientSession(headers={"User-Agent": "ITMO-PlanBot/1.0"}) as session:
        results = {}
        for key, meta in PROGRAMS.items():
            results[key] = await fetch_program_plan(session, key, meta)

        with open(CACHE_PATH, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        return results

def load_cache() -> Dict[str, Dict]:
    if os.path.exists(CACHE_PATH):
        with open(CACHE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

PLANS: Dict[str, Dict] = load_cache()

# –õ–æ–≥–∏–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤
def ensure_data_loaded() -> bool:
    return bool(PLANS) and all(k in PLANS for k in PROGRAMS.keys())

def tag_course(course: str) -> List[str]:
    low = course.lower()
    tags = []
    for tag, keys in TOPIC_KEYWORDS.items():
        if any(k in low for k in keys):
            tags.append(tag)
    return tags or ["DS/ML"]

def find_program_key(text: str) -> Optional[str]:
    t = text.lower()
    if "product" in t or "–ø—Ä–æ–¥—É–∫—Ç" in t or "—É–ø—Ä–∞–≤–ª–µ–Ω" in t:
        return "aiproduct"
    if "ai" in t or "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω" in t:
        return "ai"
    return None

def answer_is_relevant(user_q: str) -> bool:
    topics = [
        "–º–∞–≥–∏—Å—Ç—Ä", "–º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä", "–¥–∏—Å—Ü–∏–ø–ª–∏–Ω", "–∫—É—Ä—Å", "—ç–ª–µ–∫—Ç–∏–≤",
        "–ø–ª–∞–Ω", "—É—á–µ–±–Ω", "—Å–µ–º–µ—Å—Ç—Ä", "—ç–∫–∑–∞–º–µ–Ω", "–≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω",
        "—Å—Ç–æ–∏–º–æ—Å—Ç", "–æ–±—â–µ–∂–∏—Ç", "–æ–Ω–ª–∞–π–Ω", "–æ—á–Ω–∞—è", "–≤–∫—Ä", "—Ç—Ä–µ–∫",
        "—Ä–æ–ª—å", "–º–µ–Ω–µ–¥–∂–µ—Ä", "ml", "ai", "data", "product", "–ø—Ä–æ–¥–∞–∫—Ç",
    ]
    low = user_q.lower()
    return any(k in low for k in topics)

def rag_answer(user_q: str, program_keys: List[str]) -> str:
    corpus = []
    for k in program_keys:
        block = PLANS.get(k, {})
        txt = block.get("raw_text", "")
        if txt:
            corpus.append((k, block["title"], txt))

    if not corpus:
        return "–ü–æ–∫–∞ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /sync, —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å —É—á–µ–±–Ω—ã–µ –ø–ª–∞–Ω—ã."

    # —Ä–∞–∑—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–∑–∞—Ü—ã –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ–º
    chunks = []
    for k, title, txt in corpus:
      for para in re.split(r"\n{2,}", txt):
            chunk = normalize_text(para)
            if len(chunk) > 50:
                chunks.append((k, title, chunk))

    choices = [c[2] for c in chunks]
    top = process.extract(user_q, choices, scorer=fuzz.token_set_ratio, limit=3)
    hits = []
    for (cand, score, idx) in top:
        k, title, chunk = chunks[idx]
        hits.append(f"‚Äî [{title}] {chunk[:450]}{'...' if len(chunk)>450 else ''} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å {score}%)")

    return "–ù–∞—à—ë–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n\n" + "\n\n".join(hits)

def recommend_electives(background_note: str, program_key: str) -> List[str]:
    prog = PLANS.get(program_key, {})
    courses = prog.get("courses", [])
    if not courses:
        return []

    low = background_note.lower()
    desired_tags = set()
    for tag, keys in TOPIC_KEYWORDS.items():
        if any(k in low for k in keys):
            desired_tags.add(tag)

    if not desired_tags:
        desired_tags = {"DS/ML", "GenAI", "NLP", "CV", "MLOps", "DataEng", "Product", "Statistics"}

    scored = []
    for c in courses:
        c_tags = set(tag_course(c))
        overlap = len(c_tags & desired_tags)
        score = overlap * 20 + fuzz.partial_ratio(background_note.lower(), c.lower()) / 10.0
        scored.append((score, c, list(c_tags)))

    scored.sort(reverse=True, key=lambda x: x[0])

    top = []
    seen = set()
    for score, c, tags in scored:
        if c.lower() not in seen:
            seen.add(c.lower())
            top.append(f"‚Ä¢ {c} [{', '.join(tags)}]")
        if len(top) >= 12:
            break
    return top

# –•—ç–Ω–¥–ª–µ—Ä—ã –∫–æ–º–∞–Ω–¥
@dp.message(Command("start"))
async def start_cmd(message: Message):
    txt = (
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –ø–æ–º–æ–≥—É —Å—Ä–∞–≤–Ω–∏—Ç—å –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—ã –ò–¢–ú–û:\n"
        "‚Äî –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (AI)\n"
        "‚Äî AI Product (–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏)\n\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "‚Ä¢ /sync ‚Äî –∑–∞–≥—Ä—É–∑–∏—Ç—å/–æ–±–Ω–æ–≤–∏—Ç—å —É—á–µ–±–Ω—ã–µ –ø–ª–∞–Ω—ã (PDF ‚Üí —Ç–µ–∫—Å—Ç)\n"
        "‚Ä¢ /plan ai ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã AI\n"
        "‚Ä¢ /plan aiproduct ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã AI Product\n"
        "‚Ä¢ /reco <—Ç–≤–æ–π –±—ç–∫–≥—Ä–∞—É–Ω–¥/–∏–Ω—Ç–µ—Ä–µ—Å—ã> ‚Äî –ø–æ–¥–±–µ—Ä—É —ç–ª–µ–∫—Ç–∏–≤—ã\n"
        "‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π /passport ai –∏–ª–∏ /passport aiproduct, —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –ø–∞—Å–ø–æ—Ä—Ç –ø—Ä–æ–≥—Ä–∞–º–º—ã\n"
    )
    await message.answer(txt)

@dp.message(Command("sync"))
async def sync_cmd(message: Message):
    await message.answer("üîÑ –°–∫–∞—á–∏–≤–∞—é —É—á–µ–±–Ω—ã–µ –ø–ª–∞–Ω—ã –∏ –ø–∞—Ä—à—É PDF‚Ä¶")
    try:
        global PLANS
        PLANS = await sync_all()
        ai_n = len(PLANS.get("ai", {}).get("courses", []))
        pr_n = len(PLANS.get("aiproduct", {}).get("courses", []))
        await message.answer(
            f"‚úÖ –ì–æ—Ç–æ–≤–æ!\n"
            f"üìò {PROGRAMS['ai']['title']}: {ai_n} –¥–∏—Å—Ü–∏–ø–ª–∏–Ω.\n"
            f"üìô {PROGRAMS['aiproduct']['title']}: {pr_n} –¥–∏—Å—Ü–∏–ø–ª–∏–Ω."
        )
    except Exception as e:
        await message.answer(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: {e}")

@dp.message(Command("plan"))
async def plan_cmd(message: Message):
    args = message.text.split()
    if len(args) < 2:
        return await message.answer("–£–∫–∞–∂–∏ –ø—Ä–æ–≥—Ä–∞–º–º—É: /plan ai –∏–ª–∏ `/plan aiproduct`", parse_mode="Markdown")

    key = args[1].lower().replace("_", "")
    if key not in PROGRAMS:
        return await message.answer("–ù–µ –∑–Ω–∞—é —Ç–∞–∫—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É. –ò—Å–ø–æ–ª—å–∑—É–π: ai –∏–ª–∏ aiproduct.")

    if not ensure_data_loaded():
        return await message.answer("–î–∞–Ω–Ω—ã–µ –µ—â—ë –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ù–∞–∂–º–∏ /sync.")

    prog = PLANS.get(key, {})
    courses = prog.get("courses", [])
    if not courses:
        return await message.answer("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã –≤ –ø–ª–∞–Ω–µ. –ü–æ–ø—Ä–æ–±—É–π /sync.")

    title = PROGRAMS[key]["title"]
    head = f"üìö –î–∏—Å—Ü–∏–ø–ª–∏–Ω—ã ‚Äî {title}\n(–ø–æ–∫–∞–∑–∞–Ω–æ –º–∞–∫—Å–∏–º—É–º 60 —Å—Ç—Ä–æ–∫)\n\n"
    BATCH = 60
    chunks = [courses[i:i+BATCH] for i in range(0, min(len(courses), 300), BATCH)]
    for i, block in enumerate(chunks, 1):
        body = "\n".join(f"{n+1}. {c}" for n, c in enumerate(block, start=(i-1)*BATCH))
        await message.answer(head + body if i == 1 else body)

@dp.message(Command("reco"))
async def reco_cmd(message: Message):
    if not ensure_data_loaded():
        return await message
        answer("–°–Ω–∞—á–∞–ª–∞ /sync ‚Äî –Ω—É–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å –∏ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —É—á–µ–±–Ω—ã–µ –ø–ª–∞–Ω—ã.")
    text = message.text.partition(" ")[2].strip()
    if not text:
        return await message.answer(
            "–ù–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ –ø—Ä–æ —Å–µ–±—è –∏ –∏–Ω—Ç–µ—Ä–µ—Å—ã. –ü—Ä–∏–º–µ—Ä:\n"
            "/reco –û–ø—ã—Ç: Python, SQL, –∞–Ω–∞–ª–∏—Ç–∏–∫–∞; –∏–Ω—Ç–µ—Ä–µ—Å—ã: LLM, NLP, MLOps"
        )

    prog_key = find_program_key(text) or "ai"  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî —Ç–µ—Ö. AI
    items = recommend_electives(text, prog_key)
    if not items:
        return await message.answer("–ü–æ–∫–∞ –Ω–µ –º–æ–≥—É —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. –ü–æ–ø—Ä–æ–±—É–π /sync.")
    title = PROGRAMS[prog_key]["title"]
    await message.answer(f"üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —ç–ª–µ–∫—Ç–∏–≤–∞–º –¥–ª—è ¬´{title}¬ª (–¥–æ 12):\n\n" + "\n".join(items))
@dp.message(Command("passport"))
async def passport_cmd(message: Message):
    args = message.text.split()
    if len(args) < 2:
        return await message.answer("–£–∫–∞–∂–∏ –ø—Ä–æ–≥—Ä–∞–º–º—É: /passport ai –∏–ª–∏ `/passport aiproduct`", parse_mode="Markdown")

    key = args[1].lower().replace("_", "")
    if key not in PROGRAMS:
        return await message.answer("–ù–µ –∑–Ω–∞—é —Ç–∞–∫—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É. –ò—Å–ø–æ–ª—å–∑—É–π: ai –∏–ª–∏ aiproduct.")

    PASSPORTS = {
        "ai": {
            "title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (AI Talent Hub)",
            "duration": "2 –≥–æ–¥–∞",
            "form": "–æ—á–Ω–∞—è –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞ –≤ –æ–Ω–ª–∞–π–Ω-—Ñ–æ—Ä–º–∞—Ç–µ",
            "cost": "‚âà 599 000 ‚ÇΩ/–≥–æ–¥",
            "tracks": ["ML/NLP", "CV/Robotics", "Data Science"],
            "landing": PROGRAMS["ai"]["landing"]
        },
        "aiproduct": {
            "title": "AI Product (–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ò–ò-–ø—Ä–æ–¥—É–∫—Ç–∞–º–∏)",
            "duration": "2 –≥–æ–¥–∞",
            "form": "–æ—á–Ω–∞—è –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞ –≤ –æ–Ω–ª–∞–π–Ω-—Ñ–æ—Ä–º–∞—Ç–µ",
            "cost": "‚âà 599 000 ‚ÇΩ/–≥–æ–¥",
            "tracks": ["Product Management", "AI Strategy", "Data Analytics"],
            "landing": PROGRAMS["aiproduct"]["landing"]
        },
    }

    p = PASSPORTS[key]
    txt = (
        f" {p['title']}\n"
        f" –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {p['duration']}\n"
        f" –§–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è: {p['form']}\n"
        f" –°—Ç–æ–∏–º–æ—Å—Ç—å: {p['cost']}\n"
        f" –¢—Ä–µ–∫–∏/–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {', '.join(p['tracks'])}\n"
        f" –°–∞–π—Ç: {p['landing']}"
    )
    await message.answer(txt)

@dp.message()
async def any_msg(message: Message):
    q = message.text.strip()
    if not ensure_data_loaded():
        return await message.answer("–°–Ω–∞—á–∞–ª–∞ /sync ‚Äî —è –∑–∞–≥—Ä—É–∂—É —É—á–µ–±–Ω—ã–µ –ø–ª–∞–Ω—ã –∏ —Å–º–æ–≥—É –æ—Ç–≤–µ—á–∞—Ç—å –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é.")
    if not answer_is_relevant(q):
        return await message.answer(
            "–Ø –æ—Ç–≤–µ—á–∞—é —Ç–æ–ª—å–∫–æ –ø–æ –æ–±—É—á–µ–Ω–∏—é –Ω–∞ –¥–≤—É—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö –ò–¢–ú–û: "
            "¬´–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç¬ª –∏ ¬´AI Product¬ª. –ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å üôå"
        )
    ans = rag_answer(q, list(PROGRAMS.keys()))
    await message.answer(ans)

# –ó–∞–ø—É—Å–∫
async def main():
    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –ö–æ–º–∞–Ω–¥–∞: /sync ‚Äî —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–ª–∞–Ω—ã.")
    await dp.start_polling(bot)

asyncio.run(main())
